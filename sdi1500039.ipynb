{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Mining Project 2 Spring semester 2019-2020</center>\n",
    "## <center>Παναγιώτης Ευαγγελίου &emsp; 1115201500039</center>\n",
    "## <center>Γεώργιος Μαραγκοζάκης &emsp; 1115201500089</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all the necessary imports for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T21:24:51.840573Z",
     "start_time": "2020-04-29T21:24:51.836912Z"
    }
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# visualization\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# for data exploration\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Dataset Preprocessing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Make tsv files from all the txt files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "myCategoriesFolder = ['business','entertainment','politics', 'sport', 'tech']\n",
    "dataPathDir = './fulltext/data/'\n",
    "\n",
    "myDataSetDf = pd.DataFrame(columns=['ID', 'TITLE',  'CONTENT',  'CATEGORY'])\n",
    "id_count = 0\n",
    "\n",
    "for category in myCategoriesFolder:\n",
    "    specificPath = dataPathDir + category + '/'\n",
    "\n",
    "    # find the column's names of each csv\n",
    "    for fileName in os.listdir(specificPath):\n",
    "        # we need to check only .txt files\n",
    "        if fileName.endswith(\".txt\"):\n",
    "            \n",
    "            thisTxt = open(os.path.join(specificPath, fileName),\"r\")\n",
    "            thisTxtTitle = thisTxt.readline()\n",
    "\n",
    "            # get rid of '\\n' on the end of title line\n",
    "            thisTxtTitle = thisTxtTitle.replace('\\n', '')\n",
    "\n",
    "            thisTxtContent = thisTxt.readlines()\n",
    "\n",
    "            # get rid of empty lines '\\n'\n",
    "            thisTxtContent = list(filter(lambda a: a != '\\n', thisTxtContent))\n",
    "\n",
    "            # get rid of '\\n' on the end of each line \n",
    "            thisTxtContent = [period.replace('\\n', '') for period in thisTxtContent]\n",
    "\n",
    "            # convert list of lines into a single string line\n",
    "            thisTxtContent = ' '.join(thisTxtContent)\n",
    "\n",
    "            myDataSetDf = myDataSetDf.append({'ID': id_count, 'TITLE': thisTxtTitle, 'CONTENT': thisTxtContent, 'CATEGORY': category.upper()}, ignore_index=True)\n",
    "            thisTxt.close()\n",
    "\n",
    "            id_count += 1\n",
    "\n",
    "display(myDataSetDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Make wordcloud for each category__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def makeWordCloud(myText, saveLocationPath, myMaxWords=100, myMask=None, myStopWords=None):\n",
    "    '''Default function for generating wordcloud'''\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", mask=myMask, max_words=myMaxWords,\n",
    "                   stopwords=myStopWords, contour_width=3, contour_color='steelblue',\n",
    "                   width=600, height=600)\n",
    "\n",
    "    # generate word cloud\n",
    "    wc.generate(myText)\n",
    "\n",
    "    # store to file\n",
    "\n",
    "    wc.to_file(saveLocationPath)\n",
    "\n",
    "    return saveLocationPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def columnToText(myDfColumn):\n",
    "    wholeColumnText = ''\n",
    "\n",
    "    for text in myDfColumn:\n",
    "        wholeColumnText = wholeColumnText + ' ' + text\n",
    "\n",
    "    return wholeColumnText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Business Wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAdditionalStopWords = ['say','said', 'new', 'need', 'year']\n",
    "stopWords = ENGLISH_STOP_WORDS\n",
    "stopWords = (stopWords.union(myAdditionalStopWords)).union(ENGLISH_STOP_WORDS)\n",
    "makeWordCloud(saveLocationPath=\"businessWordCloud.png\", myText=columnToText(myDataSetDf[myDataSetDf['CATEGORY'] == \"BUSINESS\"]['CONTENT']), myStopWords=stopWords)\n",
    "\n",
    "Image('businessWordCloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Entertainment Wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myAdditionalStopWords = ['say','said', 'new', 'need', 'year']\n",
    "stopWords = ENGLISH_STOP_WORDS\n",
    "stopWords = (stopWords.union(myAdditionalStopWords)).union(ENGLISH_STOP_WORDS)\n",
    "makeWordCloud(saveLocationPath=\"entertainmentWordCloud.png\", myText=columnToText(myDataSetDf[myDataSetDf['CATEGORY'] == \"ENTERTAINMENT\"]['CONTENT']), myStopWords=stopWords)\n",
    "\n",
    "Image('entertainmentWordCloud.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Politics Wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myAdditionalStopWords = ['say','said', 'new', 'need', 'year']\n",
    "stopWords = ENGLISH_STOP_WORDS\n",
    "stopWords = (stopWords.union(myAdditionalStopWords)).union(ENGLISH_STOP_WORDS)\n",
    "makeWordCloud(saveLocationPath=\"politicsWordCloud.png\", myText=columnToText(myDataSetDf[myDataSetDf['CATEGORY'] == \"POLITICS\"]['CONTENT']), myStopWords=stopWords)\n",
    "\n",
    "Image('politicsWordCloud.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Sport Wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myAdditionalStopWords = ['say','said', 'new', 'need', 'year']\n",
    "stopWords = ENGLISH_STOP_WORDS\n",
    "stopWords = (stopWords.union(myAdditionalStopWords)).union(ENGLISH_STOP_WORDS)\n",
    "makeWordCloud(saveLocationPath=\"sportWordCloud.png\", myText=columnToText(myDataSetDf[myDataSetDf['CATEGORY'] == \"SPORT\"]['CONTENT']), myStopWords=stopWords)\n",
    "\n",
    "Image('sportWordCloud.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Tech Wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myAdditionalStopWords = ['say','said', 'new', 'need', 'year']\n",
    "stopWords = ENGLISH_STOP_WORDS\n",
    "stopWords = (stopWords.union(myAdditionalStopWords)).union(ENGLISH_STOP_WORDS)\n",
    "makeWordCloud(saveLocationPath=\"techWordCloud.png\", myText=columnToText(myDataSetDf[myDataSetDf['CATEGORY'] == \"TECH\"]['CONTENT']), myStopWords=stopWords)\n",
    "\n",
    "Image('techWordCloud.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Classification__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Classification using SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def SvmClassification(trainX, trainY, testX, testY, labelEncoder):\n",
    "    \"\"\"\n",
    "    Classify the text using the SVM classifier of scikit-learn    \n",
    "    \"\"\"\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "    # fit train set\n",
    "    clf.fit(trainX, trainY)\n",
    "    \n",
    "    # use 10-fold Cross Validation\n",
    "\n",
    "    print('----Report for 10-fold Cross Validation----')\n",
    "\n",
    "    # skf = StratifiedKFold(n_splits=10)\n",
    "    # precisions = cross_val_score(clf, trainX, trainY, cv=skf, scoring='precision_weighted')\n",
    "\n",
    "    precisions = cross_val_score(clf, trainX, trainY, cv=10, scoring='precision_weighted')\n",
    "    print ('Precision ', np.mean(precisions))\n",
    "\n",
    "    recalls = cross_val_score(clf, trainX, trainY, cv=10, scoring='recall_weighted')\n",
    "    print ('Recalls ', np.mean(recalls))\n",
    "\n",
    "    f1s = cross_val_score(clf, trainX, trainY, cv=10, scoring='f1_weighted')\n",
    "    print ('F-Measure ', np.mean(f1s))\n",
    "\n",
    "    scores = cross_val_score(clf, trainX, trainY, cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    # Predict test set\n",
    "    predY = clf.predict(testX)\n",
    "\n",
    "    # Classification_report\n",
    "    print('----Report for predictions on test dataset----')\n",
    "    print(classification_report(testY, predY, target_names=list(labelEncoder.classes_)))\n",
    "\n",
    "    print('----ROC plot for predictions on test dataset----')\n",
    "    return accuracy_score(testY, predY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Classification using Random Forests classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Classification using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Classification using K-Nearest Neighbor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### *Split DataSet into TrainData and TestData*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataSet, testDataSet = train_test_split(myDataSetDf, test_size=0.2, stratify=myDataSetDf['CATEGORY'])\n",
    "\n",
    "# reset index\n",
    "trainDataSet.reset_index(drop=True, inplace=True)\n",
    "testDataSet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save to tsv files\n",
    "trainDataSet.to_csv('train_set.tsv', sep = '\\t')\n",
    "\n",
    "# save test_set categories\n",
    "testDataSetCategories = testDataSet[['CATEGORY']].copy()\n",
    "\n",
    "testDataSetCategories.to_csv('test_set_categories.tsv', sep = '\\t')\n",
    "\n",
    "testDataSet = testDataSet.drop('CATEGORY', axis=1)\n",
    "testDataSet.to_csv('test_set.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train and test data that we will need below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build label encoder for categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainDataSet[\"CATEGORY\"])\n",
    "\n",
    "# transform categories into numbers\n",
    "trainY = le.transform(trainDataSet[\"CATEGORY\"])\n",
    "testY = le.transform(testDataSetCategories[\"CATEGORY\"])\n",
    "\n",
    "accuracyDict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Vectorization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do classification using 2 different ways of vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Bag-of-words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowVectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "trainX = bowVectorizer.fit_transform(trainDataSet['CONTENT'])\n",
    "testX = bowVectorizer.transform(testDataSet['CONTENT'])\n",
    "\n",
    "print('-------------SVM Classification with BOW Vectorization-------------')\n",
    "accuracyDict[\"BOW-SVM\"] = SvmClassification(trainX, trainY, testX, testY, le)\n",
    "#accuracyDict[\"BOW-SVM\"] = SvmClassification(trainX, trainY, trainX, trainY, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdfVectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "trainX = tfIdfVectorizer.fit_transform(trainDataSet['CONTENT'])\n",
    "testX = tfIdfVectorizer.transform(testDataSet['CONTENT'])\n",
    "\n",
    "print('-------------SVM Classification with TfIdf Vectorization-------------')\n",
    "accuracyDict[\"TfIdf-SVM\"] = SvmClassification(trainX, trainY, testX, testY, le)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "region,endregion",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.4.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
